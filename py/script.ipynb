{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c2c0ec749a7ab749d20bce4e650e79d915a961d"},"cell_type":"markdown","source":"## Raw Data Visualizations and Null Value Dropping"},{"metadata":{"_uuid":"33a0ff9e781f025a9430955d2c4618626692eb24"},"cell_type":"markdown","source":"Since this is a huge dataset, we'll only be able to work with a fraction of the data. The whole dataset is 55 million rows, we'll work with 3 million rows.\n\nWe'll first check for any oddities in the data, before we transform it into something we can work with."},{"metadata":{"trusted":true,"_uuid":"6646c50bb2133253c30b45be077ee01c761d4afe","collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', nrows = 3000000)\ntest = pd.read_csv('../input/test.csv')\ntrain.head(13)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b8268855b81af1b7929c0446fbb56bf0342976e"},"cell_type":"markdown","source":"Immediately we notice something odd: there's a row with all the spatial coordinates equal to 0. Let's see how many times this occurs in our sample of 5.5M rows, and if there's any other information we can get about those rows:"},{"metadata":{"trusted":true,"_uuid":"1751415b7d32a431611f8f6cb3fcb57e39f9c4cf","collapsed":true},"cell_type":"code","source":"train[(train['pickup_longitude'] == 0) | (train['pickup_latitude'] == 0) | (train['dropoff_longitude'] == 0) | (train['dropoff_latitude'] == 0)].head()\n# We'll check if any coordinate is equal to 0. Nowhere in the continental United States does lat or long = 0, so having 0 lat or long anywhere is pretty unreasonable.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3232513b46d65889a04fb053544029e094393504","collapsed":true},"cell_type":"code","source":"print('Ratio of null rows: {}'.format(len(train[(train['pickup_longitude'] == 0) | (train['pickup_latitude'] == 0) | (train['dropoff_longitude'] == 0) | (train['dropoff_latitude'] == 0)]) / len(train)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7435d0e3d27e95b614d32f23c6129ca77c7ea591","collapsed":true},"cell_type":"code","source":"ax = train[(train['pickup_longitude'] == 0) | (train['pickup_latitude'] == 0) | (train['dropoff_longitude'] == 0) | (train['dropoff_latitude'] == 0)]['fare_amount'].plot.hist(bins = 50)\nax.set_title('Fare Amounts for Null Rows')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fb4f1db0f496c1b87e92bd3c1d265e74d5dfc79"},"cell_type":"markdown","source":"So based on that analysis, here's what we know:\n* About 2% of the rows contain null spatial data\n* Those rows still contain valid target values (fare_amount), which are mostly small values but include some large/very large amounts, as well as some negative amounts.\n\nGiven this information, I think it would make the most sense to drop these rows from the dataset. Although 2% of the data is nontrivial, the lack of spatial information makes these data points difficult to work with. The spatial data is going to be key in predicting the target, and I would not want to skew the correct data by imputing these null rows with incorrect data. Therefore, we will drop these rows."},{"metadata":{"_uuid":"48e036fafaeaf8f04035b75d9b0188fd78412b44"},"cell_type":"markdown","source":"Let's visualize some other features in the raw dataset:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"8ab00e233b7d9fd84de4fba0fdd76807e96b5b64","collapsed":true},"cell_type":"code","source":"ax = train['fare_amount'].plot.hist(bins = 50)\nax.set_title('Fare Amounts Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c060850f076eaabcde3c3bdbebcd33b35e627dd","collapsed":true},"cell_type":"code","source":"train['fare_amount'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21ed212a13d557380ec341a60f1f03d5b652a3f3"},"cell_type":"markdown","source":"Looks as expected, fares mostly below $20.\n\nSome oddities include:\n* There are negative fares. It's not really clear how to interpret a negative fare, so we will most likely drop those rows from the dataset.\n* The maximum fare is $1200. Based on the histogram that seems like an outlier, but we will have to inspect the dataset further to see what's going on with that point."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3ba5714fef0aa1ccd2adc135da783f3ff39137d0","collapsed":true},"cell_type":"code","source":"train['passenger_count'].value_counts().sort_index().plot.bar(title = 'Distribution of Passenger Counts')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d7da6dd4f262a0664e6b4c3bdc138c93dfce6ea"},"cell_type":"markdown","source":"We see a non-trivial number of trips with 0 passengers, and some trips with 51, 129, or 208 passengers."},{"metadata":{"trusted":true,"_uuid":"63045c0852358f3fb9b9120b85fccfa5a85b1936","collapsed":true},"cell_type":"code","source":"train[train['passenger_count'] == 0].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c1a244a629dc33097b62188695ed909b304c4b5"},"cell_type":"markdown","source":"Aside from the passenger count, nothing seems too off about these rows. The fares and distances traveled are pretty normal-looking. \n\nOne theory I've seen is that customers will call a taxi, which starts the meter running, then cancel before they get picked up, thus racking up a fare without ever being a passenger. We can check later if the distribution of distances/fares/other metrics are different for 0 passenger trips versus non-zero passenger trips. This will help us decide if we want to leave 0 passenger counts as a level in this category, or perhaps impute it with 1.\n\nTrips with 0, 51, 129, or 208 passengers are certainly odd, but if passenger count doesn't have an effect on the fare then we don't need to worry about this feature. We'll do more inspection later."},{"metadata":{"_uuid":"3992b11e051b9634d4320fc47da8883069b1f173"},"cell_type":"markdown","source":"Let's see the spatial distribution of the datapoints:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"9e027a1c530e4e080c847a2891b0fcec5e30ac04","collapsed":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(1,2, figsize = (14,5))\ntrain_nonzero = train.drop(list(np.where((train['pickup_longitude'] == 0) | \n                          (train['pickup_latitude'] == 0) | \n                          (train['dropoff_longitude'] == 0) | \n                          (train['dropoff_latitude'] == 0))[0]), axis = 0)\ntrain_nonzero['dropoff_longitude'].describe()\nax1.scatter(x = train_nonzero['pickup_longitude'], y = train_nonzero['pickup_latitude'])\nax2.scatter(x = train_nonzero['dropoff_longitude'], y = train_nonzero['dropoff_latitude'])\nax1.set_title('Pickup Coordinates')\nax2.set_title('Dropoff Coordinates')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"283689b257e357f26e971ae0285f9623ef502160"},"cell_type":"markdown","source":"This doesn't seem right. Looking at the data, we can tell that NYC coordinates are centered around latitude 41 longitude -74, so it's odd to see points ranging from -3000:3000.\n\nOne thing to note is that we need a metric for deciding what coordinates are outliers. We can't just look at a histogram plot to decide what an outlier is. So we need to decide now how we're going to indentify coordinate outliers.\n\nThe way we'll do that is using the test set. If we look at the distributions of the coordinates in the test set, they're totally normal:"},{"metadata":{"trusted":true,"_uuid":"253ee3d94e9883ba67261c91094f8cecc3382df6","collapsed":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(1,2, figsize = (12,6))\nax1.scatter(x = test['pickup_longitude'], y = test['pickup_latitude'])\nax2.scatter(x = test['dropoff_longitude'], y = test['dropoff_latitude'])\nax1.set_title('Pickup Coordinates')\nax2.set_title('Dropoff Coordinates')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d8aa51375c26d40c8ba3903b348e10a18a17624"},"cell_type":"markdown","source":"So, we'll use the max/min coordinates of the test set as our guidelines for what isn't an outlier. To make sure we're not restricting ourselves too much, we'll add a buffer of 1.4 lat/long units on every end (although this isn't a huge buffer, the vast majority of datapoints occur within this region). This amounts to a buffer of about 100 miles on each end, which is pretty generous, considering we expect the vast majority of our data to be centered in New York City. The following code will trim the fat:"},{"metadata":{"trusted":true,"_uuid":"983e531c952ffd319aca952d3c4a6cd55b367196","collapsed":true},"cell_type":"code","source":"buffer = 1.4\nx_max = max(test['pickup_longitude'].max(),test['dropoff_longitude'].max()) + buffer\nx_min = min(test['pickup_longitude'].min(),test['dropoff_longitude'].min()) - buffer\ny_max = max(test['pickup_latitude'].max(),test['dropoff_latitude'].max()) + buffer\ny_min = min(test['pickup_latitude'].min(),test['dropoff_latitude'].min()) - buffer\n\ntrain_non0 = train.drop(list(np.where((train['pickup_longitude'] == 0) | \n                          (train['pickup_latitude'] == 0) | \n                          (train['dropoff_longitude'] == 0) | \n                          (train['dropoff_latitude'] == 0))[0]), axis = 0)\ntrain_non0.reset_index(inplace = True)\ntrain_non0.drop('index', axis=1, inplace=True)\n\noutliers_index = list(np.where(\n    (train_non0['pickup_longitude'] > x_max) | (train_non0['dropoff_longitude'] > x_max) |\n    (train_non0['pickup_longitude'] < x_min) | (train_non0['dropoff_longitude'] < x_min) |\n    (train_non0['pickup_latitude'] > y_max) | (train_non0['dropoff_latitude'] > y_max) |\n    (train_non0['pickup_latitude'] < y_min) | (train_non0['dropoff_latitude'] < y_min)\n)[0])\ntrain_outliers = train_non0.drop(index = outliers_index)\n\nfig, (ax1,ax2) = plt.subplots(1,2, figsize = (14,5))\nax1.scatter(x = train_outliers['pickup_longitude'], y = train_outliers['pickup_latitude'])\nax2.scatter(x = train_outliers['dropoff_longitude'], y = train_outliers['dropoff_latitude'])\nax1.set_title('Pickup Coordinates')\nax2.set_title('Dropoff Coordinates')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f01e41ac6e55c845eeb03042517a61d0ced91593"},"cell_type":"markdown","source":"Now the coordinates look about right. We sacrificed some data, but it's more important that we have a large amount of useable data."},{"metadata":{"_uuid":"9c6dfd7228239d4b663f345adf38c4ddacbf5e3e"},"cell_type":"markdown","source":"## Data Processing and Preliminary Feature Extraction"},{"metadata":{"_uuid":"a77a9013fdaa8f36321bf141f8705237268886c9"},"cell_type":"markdown","source":"That's about all we can visualize without doing further processing on the data.\nSince this is a huge dataset, we're going to create a processing function that can create some features from the dataset piecewise, so that we don't have the read in all the data at once. Since the preprocessing is meant to be done in chunks, there are no aggregation features done here, only features that could be extracted from a single row.\nSome features that this function extracts:\n* Drop rows with empty (0) coordinates\n* Drop rows with NaN's\n* Drop rows with coordinates too far outside of NYC\n* Drop rows with negative fare\n* Converts string date to datetime\n* Extract the year, month, day, dayofweek, and hour as features\n* Extract the vector between pickup and dropoff points\n* Get the manhattan distance and euclidean distance between pickup and dropoff (may do haversine also)\n* Get the angle of the direction vector (relative to east). For ML, this will be encoded as sine and cosine of said angle. Also extracts the compass direction of the vector, in case it's useful.\n* Calculate the fare per distance of the trip, which can be used as a crude pricing metric."},{"metadata":{"trusted":true,"_uuid":"833d061048f8a538a1dc9a30043763bb1d26c6ff","collapsed":true},"cell_type":"code","source":"## Preprocessing function\n## Apply this function to the dataset/subset of the dataset\n## Transformations that can be done elementwise (and don't require aggregations of the whole dataset) occur here\ndef data_transform1(df):\n    ## Reset the index\n    df.reset_index(inplace = True)\n    df.drop('index', axis = 1, inplace = True)\n    ## Drop NA's\n    df.dropna(axis = 0, how = 'any', inplace = True)\n    df.reset_index(inplace = True)\n    df.drop('index', axis = 1, inplace = True)\n    ## Turns out some rows contain 0 for the latitude/longitude data. Since these rows are of no use to us, we will drop them\n    df.drop(index = list(np.where((df['pickup_longitude'] == 0) | \n                          (df['pickup_latitude'] == 0) | \n                          (df['dropoff_longitude'] == 0) | \n                          (df['dropoff_latitude'] == 0))[0]), inplace = True)\n    df.reset_index(inplace = True)\n    df.drop('index', axis = 1, inplace = True)\n    ## Define conditions for outliers. test must already be defined in memory. buffer is flexible.\n    buffer = 1.4\n    x_max = max(test['pickup_longitude'].max(),test['dropoff_longitude'].max()) + buffer\n    x_min = min(test['pickup_longitude'].min(),test['dropoff_longitude'].min()) - buffer\n    y_max = max(test['pickup_latitude'].max(),test['dropoff_latitude'].max()) + buffer\n    y_min = min(test['pickup_latitude'].min(),test['dropoff_latitude'].min()) - buffer\n    ## Drop outliers\n    df.drop(index = list(np.where(\n        (df['pickup_longitude'] > x_max) | (df['dropoff_longitude'] > x_max) |\n        (df['pickup_longitude'] < x_min) | (df['dropoff_longitude'] < x_min) |\n        (df['pickup_latitude'] > y_max) | (df['dropoff_latitude'] > y_max) |\n        (df['pickup_latitude'] < y_min) | (df['dropoff_latitude'] < y_min)\n        )[0]), inplace = True)\n    df.reset_index(inplace = True)\n    df.drop('index', axis = 1, inplace = True)\n    ## Drop rows with negative fare\n    df.drop(list(np.where(df['fare_amount'] < 0)[0]), inplace = True)\n    df.reset_index(inplace = True)\n    df.drop('index', axis = 1, inplace = True)\n    ## Convert to datetime\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], format = '%Y-%m-%d %H:%M:%S %Z')\n    ## Create new features based on datetime\n    df['year'] = df['pickup_datetime'].map(lambda x:x.year)\n    df['month'] = df['pickup_datetime'].map(lambda x:x.month)\n    df['day'] = df['pickup_datetime'].map(lambda x:x.day)\n    df['dayofweek'] = df['pickup_datetime'].map(lambda x:x.dayofweek)\n    df['hour'] = df['pickup_datetime'].map(lambda x:x.hour)\n    ## Create distance vectors that we will use to create other features - these features will be dropped probably\n    df['x_diff'] = df['dropoff_longitude'] - df['pickup_longitude']\n    df['y_diff'] = df['dropoff_latitude'] - df['pickup_latitude']\n    ## Manhattan Distance\n    df['manhattan_distance'] = df['y_diff'].abs() + df['x_diff'].abs()\n    ## Euclidean Distance\n    df['distance'] = np.sqrt(df['y_diff'] ** 2 + df['x_diff'] ** 2)\n    ## Fare per distance\n    df['fare_per_distance'] = df['fare_amount'].divide(df['distance'])\n    ## Direction. After some research, it appears the best way to encode this feature for machine learning is with two features:\n    ## The sine and cosine of the angle (relative to east).\n    df['cosine'] = df['x_diff'].divide(df['distance'])\n    df['sine'] = df['y_diff'].divide(df['distance'])\n    ## We will also create an angle feature, just in case it's useful later.\n    df['angle'] = (np.arccos(df['cosine']) * (180 / np.pi) * df['y_diff'].map(lambda x: 1 if x == 0 else np.sign(x)) + 360) % 360\n    ## We can also convert an angle from degrees into a compass direction.\n    df['compass_direction'] = df['angle'].map(degree_to_compass)\n    \n\ndef degree_to_compass(angle):\n    if np.isnan(angle):\n        return(np.nan)\n    dirs = ['E','ENE','NE','NNE','N','NNW','NW','WNW','W','WSW','SW','SSW','S','SSE','SE','ESE']\n    ix = int((angle + 11.25) / 22.5 - 0.02)\n    return dirs[ix % 16]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"data_transform1(train)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4280bec8d0232cc5880b34b252b1f534d9188d11"},"cell_type":"markdown","source":"One thing I discovered while writing that function is there are some rows where the distance traveled is 0."},{"metadata":{"trusted":true,"_uuid":"ea77f28c3c0cb93295a4f2d264125676de6c53d6","scrolled":true,"collapsed":true},"cell_type":"code","source":"train[train['distance'] == 0].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2dfd12172d2100ef479a12cf1677d259775521d8"},"cell_type":"markdown","source":"In these cases the angle gets reported as NaN, since there is no angle. In how many rows does this occur:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"89dbad3678f8cd75ed24f3159b10f3de45f96aab","collapsed":true},"cell_type":"code","source":"print('Out of {} rows, {} have distance of 0'.format(len(train), len(train[train['distance'] == 0])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"045924cc845ec6d189c7be70e106e58381bf7cd6"},"cell_type":"markdown","source":"So about 1% of rows have this phenomenon. Let's see if there's anything peculiar about those rows:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b5593a71580390250cd71de6379b57304c888e5b","collapsed":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(1,2, figsize = (10,3))\nax1.hist(train[train['distance'] == 0]['fare_amount'], bins = 50)\nax2.hist(train[train['distance'] != 0]['fare_amount'], bins = 50)\nax1.set_title('Fares with distance 0')\nax2.set_title('Fares with distance >0')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69c18346f4586917469ac5ef80d756da668a51c1"},"cell_type":"markdown","source":"We can't see much of a difference in the rows with distance 0. It might make sense to drop these rows like we dropped the null rows (especially since there are so few of them). However, these rows do actually contain spatial data and fares, so they may be useful. We'll keep these rows for now, and we'll decide what to do with them after a bit more analysis."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5fcd41e5791fbb6dffde03a9bda7acbf2fd7bc02"},"cell_type":"markdown","source":"Let's check some other univariate distribution statistics to try to find anything interesting:"},{"metadata":{"trusted":true,"_uuid":"7292fdc5afb0cbad550b1824c5e6dfaaaffa43ab","collapsed":true},"cell_type":"code","source":"fig, (ax1,ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize = (16,5))\nax1.bar(train['year'].unique(), train.groupby('year').mean()['fare_amount'].values)\nax2.plot(train[(train['distance'] > 0.05) & (train['distance'] < 0.6)].groupby(['year','month']).mean()['fare_per_distance'].values)\nax3.plot(train[(train['distance'] > 0.05) & (train['distance'] < 0.6)].groupby('hour').mean()['fare_per_distance'].values)\nax4.plot(train[(train['distance'] > 0.05) & (train['distance'] < 0.6)].groupby('dayofweek').mean()['fare_per_distance'].values)\nax5.hist(train['distance']*69, bins = 50)\nax1.set(title = 'Mean fare per year', xlabel = 'Year')\nax2.set(title = 'Mean rate per month, 2009-2015')\nax3.set(title = 'Mean rate by hour', xlabel = 'Hour of day')\nax4.set(title = 'Mean rate by day of week', xlabel = 'Day of Week')\nax5.set(title = 'Distribution of trip distance', xlabel = 'Distance (mi., approx)', xlim = (0,20))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"764467b437f75d686e39298aa0d151d1bcebf594"},"cell_type":"markdown","source":"Here, rate is the fare divided by the distance travelled in a trip. This value may be skewed due to some long-distance/low-fare trips (which is why in the code I restricted distances to under 40 miles for this graph), but we will later see that most of the data follows a linear trend.\n\nWhat we can see from these graphs:\n* The mean trip price has been increasing each year, more or less. Based on the first graph, we will probably want to use the year as a feature in our model.\n* The mean trip rate per month has an interesting jump right in the middle - maybe the taxi company raised their prices that month. There doesn't appear to be a strong seasonality so we won't use a time series for our model. But maybe we should add a variable for 'After September 2012', since that increase in fare rate is non-trivial.\n* There is a clear seasonality in the fare per distance metric by hour of the day. Fares get a lot more expensive around 9AM until 6PM. We will probably want to use a sine transform of the hour of the day to capture this feature.\n* There appears to be a dependence of the rate on the day of the week. But if you look at the y-axis of that graph, the range in values is so small compared to the other 2 that the effect is not worth trying to model, as we may overfit by introducing this feature.\n\nThere are some outliers in distance that we can see here:"},{"metadata":{"trusted":true,"_uuid":"5798bf6c768ccee8f26c5d60f6a5ced571c6f8ef","collapsed":true},"cell_type":"code","source":"train['distance'].sort_values().tail() * 69","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e5ce7c120e64aa4bbac00a02056ba320627463d"},"cell_type":"markdown","source":"Some trips that hit the boundary of our defined region. These are pretty long for taxi rides, but not totally unreasonable."},{"metadata":{"_uuid":"ba4730d334e9ab68714a7e82fd1b05f08a3ebaf3"},"cell_type":"markdown","source":"Now let's try visualizing some interactions between variables.\nThe most fundamental interaction we would expect to see is a positive correlation between ride distance and fare amount. We'll start with a scatter plot of those variables:"},{"metadata":{"trusted":true,"_uuid":"7165f645dea2a9f956346adf96bc790099c9c659","collapsed":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (14,6))\ng = sns.regplot(x = 'distance', y = 'fare_amount', data = train, ax = ax)\ng.set_title('Fares vs. Distance')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25df6baa3fd30e9ba5d7ce8c7204543f0ecccd25"},"cell_type":"markdown","source":"The bulk of the data follows a linear trend.\n\nThe longer trips (>0.8 distance, or about 50mi.+) all have constant fare. It's likely that these trips are all fixed-fare trips: for example, a trip to the airport may be constant regardless of the distance. We will probably want to introduce a 'long_trip' flag for trips longer than 0.8, as these trips are most likely not going to follow the linear trend (there are a small number of exceptions that you can see, but out of 2 million points it's a very small number). However, the length of the trip may not be the only metric that indicates a constant fare. Pickup and dropoff locations are probably also important.\n\nIn addition, those distance 0 trips are throwing off the data a bit. The fares range from 0 to $500, and without knowing the distance traveled they aren't contributing to this particular model.\n\nThere's also some short trips with fares of over $400. I'm not sure how that happens, so we might drop those points as outliers.\n\nWhat would our regression plot look like if we dropped the distance 0 trips, and the longer constant-fare trips:"},{"metadata":{"trusted":true,"_uuid":"3b089bb7e0830ad911285d459371e9dabb3836e7","collapsed":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (14,6))\ng = sns.regplot(x = 'distance', y = 'fare_amount', data = train[(train['distance'] > 0.02) & (train['distance'] < 0.55) & (train['fare_amount'] > 1) & (train['fare_amount'] < 220)], ax = ax)\ng.set_title('Fares vs. Distance (No outliers)')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afb6bfd8573b982af0a6297298717c5f45d63c16"},"cell_type":"markdown","source":"It's messy, but there is a linear trend for datapoints within this range.\n\nWe can see some low-fare trips that range pretty large distances (30 miles for under $10 for example), which seem like they will be difficult to predict if we are only given the start and end coordinates.\n\nFor completeness, let's see if there is a similar or better trend when using the manhattan distance:"},{"metadata":{"trusted":true,"_uuid":"e9721d8e2773ab54c514731e3fda5038cb4e214f","collapsed":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (14,6))\ng = sns.regplot(x = 'manhattan_distance', y = 'fare_amount', data = train[(train['manhattan_distance'] > 0.02) & (train['manhattan_distance'] < 0.65) & (train['fare_amount'] > 1) & (train['fare_amount'] < 220)], ax = ax)\ng.set_title('Fares vs. Manh. Distance (No outliers)')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92c6617a72482a261d022c239c266e3483a4d867"},"cell_type":"markdown","source":"Looks about the same. It's not clear if euclidean or manhattan distance (or both, or a weighted average, etc) will produce a better model. We will have to wait till the modeling stage."},{"metadata":{"_uuid":"d6610f47ca2cb9d0832c773d66aaca4f448d095b"},"cell_type":"markdown","source":"Let's see how some metrics vary by the number of passengers. Recall the distribution of passenger counts:"},{"metadata":{"trusted":true,"_uuid":"fc836a16fa659bbd847d26788da8faf7726a26bf","collapsed":true},"cell_type":"code","source":"train['passenger_count'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d636872d73723aa46a2cd78647c8adcec6ea9ad7"},"cell_type":"markdown","source":"There's some pretty clear outliers here. We will first check the non-outlier values:"},{"metadata":{"trusted":true,"_uuid":"9a842a07317c1a15956faf8ef0a305f2343f89c2","collapsed":true},"cell_type":"code","source":"f, (ax1,ax2) = plt.subplots(1,2, figsize = (14,5))\nfor p in list(range(0,7)):\n    sns.kdeplot(train[train['passenger_count'] == p]['distance']*69, ax = ax1, label = \"Passengers: {}\".format(p))\n    sns.kdeplot(train[train['passenger_count'] == p]['fare_amount'], ax = ax2, label = \"Passengers: {}\".format(p))\nax1.set(xlabel = 'Distance (mi., approx)', title = 'KDE of Distance by # Passengers', xlim = (0,6))\nax2.set(xlabel = 'Fare Amount (USD)', title = 'KDE of Fare Amount by # Passengers', xlim = (0,25))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"952c7a6d8b086d263836fce55460c326109c25f7","collapsed":true},"cell_type":"code","source":"train[train['distance'] > 0.05].groupby('passenger_count').mean()['fare_per_distance']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7292170d485772a297422133ffd000d3634edf42"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f6044cc1feb1b6cf7da004cb6391303dfb85b74"},"cell_type":"markdown","source":"Based on these, we can see:\n* With the exception of 0-passenger trips, distance travelled per trip seems inversely related to the number of passengers (1 and 2 passengers travel longer distances)\n* In correspondance with the previous bullet, 1 and 2 passenger trips tend to pay more per trip (because they travel further)\n* However, the mean fare per distance travelled is roughly the same for all passenger counts.\n\nSo, it appears that the number of passenger doesn't actually affect the fare. The only correlation is between distance travelled and total fare, and the rate of this correlation doesn't depend on the number of passengers. Therefore, we can probably ignore the number of passengers when we build a model."},{"metadata":{"_uuid":"e9fc779ffe56a3f61980fa28f12c2f2ebcc475a9"},"cell_type":"markdown","source":"Next we'll try and visualize the actual locations of pickup and dropoff points using the coordinates. We will do this using a 2d hexplot.\nThe objective is to try to find distinct \"hotspots.\" If we can find some groups of locations with similar fare values or trends, then we can create bins to sort the pickup and dropoff locations into, and hopefully those bins act as better features than the coordinates themselves."},{"metadata":{"trusted":true,"_uuid":"3cce4326dd90f8deb9ce1a31785ed27bdf7e2aee","collapsed":true},"cell_type":"code","source":"sns.jointplot(train['pickup_longitude'], train['pickup_latitude'], kind = 'kde', bins = 1000, xlim = (-74.1,-73.8), ylim = (40.6,41.0))\nsns.jointplot(train['dropoff_longitude'], train['dropoff_latitude'], kind = 'kde', bins = 1000, xlim = (-74.1,-73.8), ylim = (40.6,41.0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"025b4942352cae147d64decd1a4bed9069c29990"},"cell_type":"markdown","source":"These plots alone don't quite tell us what we need to know about the location data. A high degree of sophistication is required to work with the coordinates; I'll come back to it when I have a better idea of how to glean knowledge from them."},{"metadata":{"_uuid":"668dc973ce7a0035fdcb3a75e46a4f639449dbd9"},"cell_type":"markdown","source":"## Examining the Test Set"},{"metadata":{"trusted":true,"_uuid":"ca93a77e4ed130edf645074aeaa8e45a83f7762c","collapsed":true},"cell_type":"markdown","source":"We're almost ready to create our first version of a model. However, after completing our analyses we still have some points we need to address when modeling, such as:\n* How do we handle the number of passengers (esp. outliers like 0 passengers or >6 passengers)\n* How do we handle distances of 0\n* How do we handle distances >30 miles (at which point the linear relation between fare and distance ceases to exist)\n* Should we model with euclidean distance or manhattan distance (or both)?\n\nTo answer these questions, we'll start by looking at the test set. We want to see the distribution of data points in the test set, so we can try to model our interpretation of the training set around it.\n\nThe first thing we need to do is apply our transformation function to the test set to get the distances and other features."},{"metadata":{"trusted":true,"_uuid":"d5f96624ae309b950f092a4951b07e1258f19634","collapsed":true},"cell_type":"code","source":"## Have to give a 'fare_amount' column to the test set so the function will not error.\ntest['fare_amount'] = 0\ndata_transform1(test)\ntest['fare_amount'] = np.nan\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"760c2db2d70e48b824dbe0bfe276b3ed723d9bf2"},"cell_type":"markdown","source":"Distribution of passenger count:"},{"metadata":{"trusted":true,"_uuid":"37d80485a467550aa3ffe15824baefc07d61f0f1","collapsed":true},"cell_type":"code","source":"test['passenger_count'].value_counts().sort_index().plot.bar(title = 'Distribution of Passenger Counts')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab34edf105dfccae720f5c3cc7c24900079a6663"},"cell_type":"markdown","source":"No 0-passenger rides present in the test set. But we probably aren't going to use passenger count in our model anyways."},{"metadata":{"_uuid":"6639404143b038eb5cf852262411e7c31d774ca0"},"cell_type":"markdown","source":"Distribution of distances of trips:"},{"metadata":{"trusted":true,"_uuid":"dbeda58655a27a2da8a779976d0573f4b44389a4","collapsed":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(1,2,figsize = (12,4))\nax1.hist(test['distance']*69, bins = 30)\nax2.hist(test['manhattan_distance']*69, bins = 40)\nax1.set(title = 'Trip Distance Distribution', xlabel = 'Distance (mi. approx)')\nax2.set(title = 'Trip Manhattan Distance Distribution', xlabel = 'Distance (mi. approx)')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97a0e00b7a7961fe41608854b438e9d59a439b32"},"cell_type":"markdown","source":"The minimum distance in the test set is:"},{"metadata":{"trusted":true,"_uuid":"76a3f3428688a94a93d21ca86ec1f48cab0342e3","collapsed":true},"cell_type":"code","source":"print('Min distance: {}'.format(test['distance'].min()))\nprint('Min manhattan distance: {}'.format(test['manhattan_distance'].min()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cf55e4e713893f8367889a5a02dc82d68343865"},"cell_type":"markdown","source":"So the test set does indeed contain some distance 0 trips. How may?"},{"metadata":{"trusted":true,"_uuid":"361b193d4829c2039970d2886dbada407756853d","collapsed":true},"cell_type":"code","source":"print('Out of {} rows, {} have distance of 0'.format(len(test), len(test[test['distance'] == 0])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a3cdf91e8a1909fa7a4f0c7d5f5a0732aed2117"},"cell_type":"markdown","source":"Like the training set, we have to deal with about 1% of rows having distance 0. Therefore, we won't drop those distance 0 trips from the training set, since we'll need them to make predictions. Instead, we'll create a 'distance0' variable that will indicate if the trip has distance 0, and see if the model can use that to make an accurate prediction."},{"metadata":{"trusted":true,"_uuid":"0741ed44522eada4d276434d86de93c2ab264274","collapsed":true},"cell_type":"code","source":"test['distance'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4abda9de8878573fefc334c3b1ef076fbd978b2c"},"cell_type":"markdown","source":"From the test set histograms we see that there are a very small number of points with distance >30 mi. There are in fact only three trips with distances greater than 30 mi., and they are all about 70 mi. Looking at the training data, most of the test points (which have distance <30 mi.) lie in the linear region of the fare vs. distance plot. The 70 mile points lie in a region that is almost entirely constant-fare. We can assume these 3 points are probably constant-fare as well, and we can create a 'long_distance' boolean variable just for these 3 points."},{"metadata":{"_uuid":"60c81ea88ba2dd3266498d9c5bdad0f3206e598c"},"cell_type":"markdown","source":"## Model V2.2"},{"metadata":{"_uuid":"343c242abdf458cca6b8ba494a4c380bf4aeaa31"},"cell_type":"markdown","source":"Model V2 introduces new Distance == 0 and Distance > 0.8 boolean variables. Model V2 will also not drop distance 0 points from the training set.\n\nModel V2.1 introduces a bias column.\n\nModel V2.2 introduces the 'After Sept 2012' variable, to reflect the jump in fare price in September 2012."},{"metadata":{"trusted":true,"_uuid":"c1c26b6a214da3c89fac7f1687ae1811e2ae3fc7","collapsed":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"105babe2c77b33efb335919379fde11d62078535"},"cell_type":"markdown","source":"We will try XGBoost for our model. The features we will use to predict are:\n* Pickup coordinates\n* Dropoff coordinates\n* Year\n* Sine transform of the hour of the day\n* Distance\n* Manhattan Distance\n* Distance == 0 (new from V1)\n* Distance > 0.8 (new from V1)\n* Cosine and Sine of direction traveled"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"86377fab399d1f350e006fe193f15ca6140e9b67"},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0b75b70126004b23fe0fd9f8add77b182d4f997f"},"cell_type":"code","source":"def data_transform2(df):\n    ## Bias column\n    df['bias'] = 1\n    ## Hour of the day variable\n    df['sine_hour'] = np.sin(2 * np.pi * df['hour'] / 24)\n    ## Distance 0 variable\n    df['distance0'] = (df['distance'] == 0)\n    ## Long distance variable\n    df['long_distance'] = (df['distance'] > 0.8)\n    ## After September 2012\n    df['after_sept2012'] = (df['year'] > 2012) | ((df['year'] == 2012) & (df['month'] >= 9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6747d03c12617754853dc144945e523cfe4b2b3","collapsed":true},"cell_type":"code","source":"## Drop the training rows where distance = 0\ntarget = train['fare_amount']\n\ncols = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','sine_hour','distance','manhattan_distance','cosine','sine']\n\n## Concatenate train and test for scaling\n## (Can de-concatenate by checking for NaN rows in fare_amount)\ndf = pd.concat([train,test], ignore_index = True, sort = False)\ntrain_index = np.where(~(pd.isnull(df['fare_amount'])))\ntest_index = np.where(pd.isnull(df['fare_amount']))\n\n## New sine-hour feature\ndata_transform2(df)\n\n## Scale\nscaler = StandardScaler()\ndf = pd.concat([pd.DataFrame(data = scaler.fit_transform(df[cols]), columns = cols),df[['year','distance0','long_distance','bias','after_sept2012']]], axis = 1)\ntrain = df.loc[train_index]\ntest = df.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbefe0603516ceeb9a112548eb321c55ca2e146c"},"cell_type":"markdown","source":"Data preprocessing is done. Let's create a model."},{"metadata":{"trusted":true,"_uuid":"867c95beb7d9459e261ac81994d21e751d1a71cd","collapsed":true},"cell_type":"code","source":"model = xgb.XGBRegressor(max_depth = 7, learning_rate = 0.012, n_estimators = 300)\nmodel.fit(X = train, y = target, eval_metric = 'rmse')\npred = model.predict(data = test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1c447b123b327737a1e0c61ac59c6d20a6c0ec6c"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = pred\nsubmission.to_csv('submissionV2_2.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b7b6e625e6ca393b6c6755be65f8813b16690a5"},"cell_type":"markdown","source":"This crude version ended up with a ~4.5 RMSE, about 1 unit better than the basic linear model. It's good that we're better than the baseline, but we should aim to do better.\n\nSome things we can do to improve the model include:\n* Measure the popularity of a pickup/dropoff point: fraction of points that are within some distance (distance will need to be tuned) of the pickup point/dropoff point (debatable if this has an effect on fare though)\n* Find the mean fare of points close to the pickup/dropoff point (again have to find if this actually impacts the fare) (this feature may encode tolls though so it could be worthwhile)\n* Distance of pickup/dropoff to airport (or, boolean for within some miles of an airport) This may encode the constant fare trips.\n* Use more data. We used about 4% of the data. Either construct models piecewise and take an average, or try to read in the whole dataset somehow.\n"},{"metadata":{"trusted":true,"_uuid":"092bbbfa6184864a82bb70f20632d18613b80cb2","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8715ed72131e5d06434a748e65571d2e25f4f6f4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}